<!doctype html>
<html>
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>CIM + Spark</title>

        <link rel="shortcut icon" href="img/favicon.ico">
		<link rel="stylesheet" href="css/reveal.css">
		<link rel="stylesheet" href="css/theme/white.css">

		<!-- Theme used for syntax highlighting of code -->
		<link rel="stylesheet" href="lib/css/github.css">

		<!-- Printing and PDF exports -->
		<script>
			var link = document.createElement( 'link' );
			link.rel = 'stylesheet';
			link.type = 'text/css';
			link.href = window.location.search.match( /print-pdf/gi ) ? 'css/print/pdf.css' : 'css/print/paper.css';
			document.getElementsByTagName( 'head' )[0].appendChild( link );
		</script>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<section data-markdown>
                    <script type="text/template">
                        <img src="img/splash_page.svg" style="border-width: 0px; background-color: transparent; box-shadow: initial;">

                        #### Network Analysis and Simulation using Apache Spark on Compute Clusters

                        ###### Derrick Oswald

                        <aside class="notes">
                            Network Analysis and Simulation using Apache Spark on Compute Clusters
                            by
                            Derrick Oswald
                        </aside>

                    </script>
                    <audio>
                        <source src="media/splash.mp3" type="audio/mp3">
                    </audio>
                </section>
                <section data-markdown>
                    <script type="text/template">
                        ## Overview

                        * CIM
                        * Apache Spark
                        * CIMReader
                        * Use-Case – Asset Reports
                        * GraphX & Pregel algorithm
                        * Use-Case – Short Circuit Current
                        * GridLAB-D
                        * Use-Case – Maximum Feed-In Power
                        * Summary

                        <aside class="notes" data-markdown>
                            This presentation covers
                            an introduction to the Common Information Model (CIM)
                            and Apache Spark, a large-scale data processing engine.

                            It then does a deeper dive into the CIMReader for Spark
                            that melds these two paradigms,
                            and presents a simple use-case for asset query.

                            An overview of network processing
                            using the Pregel algorithm from the GraphX package is presented next,
                            and illustrated by a short-circuit current use-case.

                            Lastly, the GridLAB-D application is fit into the picture
                            for non-linear load-flow,
                            with a distributed energy resource use-case.
                        </aside>
                    </script>
                    <audio>
                        <source src="media/overview.mp3" type="audio/mp3">
                    </audio>
                </section>
                <section data-markdown>
                    <script type="text/template">
                        ## CIM
                        <img src="img/CIM.svg" style="border-width: 0px; background-color: transparent; box-shadow: initial; float: right; margin-right: 2em; height:auto; max-width:50%;">

                        * immense set of standards
                        * UML data model
                        * XML/RDF specifications
                        * interchange between systems

                        <aside class="notes" data-markdown>
                            CIM comprises many IEC standards.
                            For our purposes it consists of
                            a Unified Modeling Language (UML) data model,
                            and an Extensible Markup Language (XML) file format
                            with Resource Description Framework (RDF) syntax,
                            that can be used to exchange
                            electrical network information
                            between two systems.
                        </aside>

                    </script>
                    <audio>
                        <source src="media/cim.mp3" type="audio/mp3">
                    </audio>
                </section>
                <section data-markdown>
                    <script type="text/template">
                        ## Model
                        <img src="img/model.svg" style="border-width: 0px; background-color: transparent; box-shadow: initial; float: right; margin-right: 2em; height:auto; max-width:50%;">

                        * over 1500 classes
                          * electrical equipment
                          * connectivity
                          * customer metering
                          * service and billing
                          * operations and maintenance

                        <aside class="notes" data-markdown>
                            The CIM data model has a class for nearly every germane
                            object of interest to electric utilities,
                            including:

                            - electrical equipment and connections
                            - customer metering, service and billing
                            - operations and maintenance

                            and many others.
                            The diagram shows a few of the class names
                            such as:

                            - ACLineSegment which is a cable
                            - EnergyConsumer which is a house connection
                            - and interior details at a ServiceLocation like Customer and Meter 
                        </aside>
                    </script>
                    <audio>
                        <source src="media/model.mp3" type="audio/mp3">
                    </audio>
                </section>
                <section>
                    <section data-markdown>
                    <script type="text/template">
                        ## Types of Information
                        <img src="img/information.svg" style="border-width: 0px; background-color: transparent; box-shadow: initial; float: right; margin-right: 2em; height:auto; max-width:50%;">

                        * electrical characteristics
                        * topological connectivity
                        * asset details
                        * spatial locations and addresses
                        * measurements

                        <aside class="notes" data-markdown>
                            Several types of information are encoded in the CIM file format,
                            including
                            - static and dynamic electrical characteristics
                            - topological connectivity
                            - asset details for life-cycle and maintenance
                            - spatial locations and addresses
                            - measurement time series
                            - etc.
                        </aside>
                    </script>
                    <audio>
                        <source src="media/types.mp3" type="audio/mp3">
                    </audio>
                    </section>
                    <section>
                        <div>Electrical</div>
                        <pre style="box-shadow: initial;"><code style="font-size: 12px; background-color: transparent; line-height: 12px;">
&#x2af6;
&lt;cim:SwitchInfo rdf:ID="ART4179"&gt; &lt;!-- electrical characteristics --&gt;
    &lt;cim:IdentifiedObject.name&gt;8DJH 24kV&lt;/cim:IdentifiedObject.name&gt;
    &lt;cim:IdentifiedObject.aliasName&gt;810:nis_el_int_switch_art&lt;/cim:IdentifiedObject.aliasName&gt;
    &lt;cim:IdentifiedObject.description&gt;Gas-Insulated Medium-Voltage Switchgear&lt;/cim:IdentifiedObject.description&gt;
    &lt;cim:SwitchInfo.ratedVoltage&gt;24000.0000&lt;/cim:SwitchInfo.ratedVoltage&gt;
&lt;/cim:SwitchInfo&gt;
&lt;cim:Switch rdf:ID="TEI127390"&gt;
    &lt;cim:IdentifiedObject.name&gt;8DJH 24kV&lt;/cim:IdentifiedObject.name&gt;
    &lt;cim:IdentifiedObject.aliasName&gt;276857882:nis_el_int_switch&lt;/cim:IdentifiedObject.aliasName&gt;
    &lt;cim:IdentifiedObject.description&gt;Switch&lt;/cim:IdentifiedObject.description&gt;
    &lt;cim:ConductingEquipment.BaseVoltage rdf:resource="#BaseVoltage_16000"/&gt; &lt;!-- voltage --&gt;
    &lt;cim:PowerSystemResource.Location rdf:resource="#_location_1621079552_427087358_276857884"/&gt;
    &lt;cim:PowerSystemResource.PSRType rdf:resource="#PSRType_Substation"/&gt;
    &lt;cim:PowerSystemResource.AssetDatasheet rdf:resource="#ART4179"/&gt; &lt;!-- info reference --&gt;
    &lt;cim:Equipment.EquipmentContainer rdf:resource="#FEL215886"/&gt;
    &lt;cim:Switch.normalOpen&gt;false&lt;/cim:Switch.normalOpen&gt; &lt;!-- state --&gt;
&lt;/cim:Switch&gt;
&#x2af6;
                        </code></pre>
                    </section>
                    <section>
                        <div>Spatial</div>
                        <pre style="box-shadow: initial;"><code style="font-size: 12px; background-color: transparent; line-height: 12px;">
&#x2af6;
&lt;cim:Location rdf:ID="_location_1621079552_427087358_276857884"&gt; &lt;!-- spatial location --&gt;
    &lt;cim:Location.CoordinateSystem rdf:resource="#pseudo_wgs84"/&gt;
    &lt;cim:Location.type&gt;geographic&lt;/cim:Location.type&gt;
&lt;/cim:Location&gt;
&lt;cim:PositionPoint rdf:ID="_location_1621079552_427087358_276857884_p"&gt; &lt;!-- point location --&gt;
    &lt;cim:PositionPoint.Location rdf:resource="#_location_1621079552_427087358_276857884"/&gt;
    &lt;cim:PositionPoint.sequenceNumber&gt;0&lt;/cim:PositionPoint.sequenceNumber&gt;
    &lt;cim:PositionPoint.xPosition&gt;7.22933091967&lt;/cim:PositionPoint.xPosition&gt;
    &lt;cim:PositionPoint.yPosition&gt;47.1121810794&lt;/cim:PositionPoint.yPosition&gt;
&lt;/cim:PositionPoint&gt;
&lt;cim:Switch rdf:ID="TEI127390"&gt;
    &lt;cim:IdentifiedObject.name&gt;8DJH 24kV&lt;/cim:IdentifiedObject.name&gt;
    &lt;cim:IdentifiedObject.aliasName&gt;276857882:nis_el_int_switch&lt;/cim:IdentifiedObject.aliasName&gt;
    &lt;cim:IdentifiedObject.description&gt;Switch&lt;/cim:IdentifiedObject.description&gt;
    &lt;cim:ConductingEquipment.BaseVoltage rdf:resource="#BaseVoltage_16000"/&gt;
    &lt;cim:PowerSystemResource.Location rdf:resource="#_location_1621079552_427087358_276857884"/&gt;  &lt;!-- loc. reference --&gt;
    &lt;cim:Switch.normalOpen&gt;false&lt;/cim:Switch.normalOpen&gt;
&#x2af6;
                        </code></pre>
                    </section>
                    <section>
                        <div>Topology</div>
                        <pre style="box-shadow: initial;"><code style="font-size: 12px; background-color: transparent; line-height: 12px;">
&#x2af6;
&lt;cim:Terminal rdf:ID="TEI127390_terminal_1"&gt;
    &lt;cim:IdentifiedObject.name&gt;TEI127390_terminal_1&lt;/cim:IdentifiedObject.name&gt;
    &lt;cim:ACDCTerminal.sequenceNumber&gt;1&lt;/cim:ACDCTerminal.sequenceNumber&gt;
    &lt;cim:Terminal.phases rdf:resource="http://iec.ch/TC57/2013/CIM-schema-cim16#PhaseCode.ABC"/&gt;
    &lt;cim:Terminal.ConnectivityNode rdf:resource="#PIN518369_node"/&gt; &lt;!-- connectivity reference --&gt;
    &lt;cim:Terminal.ConductingEquipment rdf:resource="#TEI127390"/&gt; &lt;!-- equipment reference --&gt;
&lt;/cim:Terminal&gt;
&#x2af6;
&lt;cim:ConnectivityNode rdf:ID="PIN518369_node"&gt; &lt;!-- connection --&gt;
    &lt;cim:IdentifiedObject.name&gt;PIN518369&lt;/cim:IdentifiedObject.name&gt;
    &lt;cim:IdentifiedObject.aliasName&gt;276857893:nis_el_int_pin&lt;/cim:IdentifiedObject.aliasName&gt;
    &lt;cim:IdentifiedObject.description&gt;TEI127390 pin 2&lt;/cim:IdentifiedObject.description&gt;
    &lt;cim:ConnectivityNode.ConnectivityNodeContainer rdf:resource="#STA212"/&gt;
&lt;/cim:ConnectivityNode&gt;
&#x2af6;
&lt;cim:Terminal rdf:ID="REF201114_terminal_1"&gt; &lt;!-- connection --&gt;
    &lt;cim:IdentifiedObject.name&gt;REF201114_terminal_1&lt;/cim:IdentifiedObject.name&gt;
    &lt;cim:ACDCTerminal.sequenceNumber&gt;1&lt;/cim:ACDCTerminal.sequenceNumber&gt;
    &lt;cim:Terminal.phases rdf:resource="http://iec.ch/TC57/2013/CIM-schema-cim16#PhaseCode.ABC"/&gt;
    &lt;cim:Terminal.ConnectivityNode rdf:resource="#PIN518369_node"/&gt; &lt;!-- connectivity reference --&gt;
    &lt;cim:Terminal.ConductingEquipment rdf:resource="#REF201114"/&gt; &lt;!-- equipment reference --&gt;
&lt;/cim:Terminal&gt;
&#x2af6;
                        </code></pre>
                    </section>
                    <section>
                        <div>Asset</div>
                        <pre style="box-shadow: initial;"><code style="font-size: 12px; background-color: transparent; line-height: 12px;">
&#x2af6;
&lt;cim:LifecycleDate rdf:ID=TEI127390_lifecycle"&gt; &lt;!-- lifecycle --&gt;
    &lt;cim:LifecycleDate.installationDate&gt;2001&lt;/cim:LifecycleDate.installationDate&gt;
&lt;/cim:LifecycleDate&gt;
&lt;cim:Asset rdf:ID="TEI127390_asset"&gt;
    &lt;cim:IdentifiedObject.name&gt;TEI127390&lt;/cim:IdentifiedObject.name&gt;
    &lt;cim:IdentifiedObject.aliasName&gt;276857882:nis_el_int_switch&lt;/cim:IdentifiedObject.aliasName&gt;
    &lt;cim:IdentifiedObject.description&gt;Line Switch&lt;/cim:IdentifiedObject.description&gt;
    &lt;cim:Asset.type&gt;8DJH 24kV&lt;/cim:Asset.type&gt;
    &lt;cim:Asset.lifecycle rdf:resource="#TEI127390_lifecycle"/&gt; &lt;!-- lifecycle reference --&gt;
    &lt;cim:Asset.Location rdf:resource="#_location_1621079552_427087358_276857884"/&gt;
    &lt;cim:Asset.PowerSystemResources rdf:resource="#TEI127390"/&gt; &lt;!-- equipment reference --&gt;
&lt;/cim:Asset&gt;
&#x2af6;
                        </code></pre>
                    </section>
                </section>
                <section data-markdown>
                    <script type="text/template">
                        ## Spark
                        <img src="img/spark.svg" style="border-width: 0px; background-color: transparent; box-shadow: initial; float: right; margin-right: 2em; height:auto; max-width:50%;">

                        * Cluster
                        * Hadoop
                        * AWS, Azure
                        * Scala, Python, R, Java

                        <aside class="notes" data-markdown>
                            Turning now from data to applications...
                            Apache Spark is software that is normally
                            - executed on a cluster
                            - usually using Hadoop,
                            - and usually in a cloud environment,
                            such as Amazon Web Services (AWS) or Microsoft Azure.
                            Out of the box, Spark provides
                            some interactive command line interpeters
                            for Scala, Python and R,
                            which can be run from a secure shell.
                            Graphical user interfaces to Spark in a browser
                            such as RStudio, Zeppelin, Jupiter, etc.
                            must be installed separately.
                            The Spark API is available for Scala, Python and R,
                            and of course for Java,
                            which it all comes down to in the end.
                            For batch oriented use-cases,
                            the spark-submit command
                            is usually executed on the master node,
                            by specifying a custom Java jar file
                            and a command line.
                        </aside>
                    </script>
                    <audio>
                        <source src="media/spark.mp3" type="audio/mp3">
                    </audio>
                </section>
                <section data-markdown>
                    <script type="text/template">
                        ## Software Stack
                        <img src="img/software_stack.svg" style="border-width: 0px; background-color: transparent; box-shadow: initial; float: right; margin-right: 2em; height:auto; max-width:50%;">

                        * Master
                        * Worker
                        * Driver
                        * Executor
                        * Name Node
                        * Data Node

                        <aside class="notes" data-markdown>
                            The main moving parts on the cluster are all
                            Java Virtual Machine instances.
                            spark-submit instructs the Spark agent on the master node,
                            to spawn the driver which will execute the command using the jar.
                            The master requests each worker agent to
                            spawn one or more executor instances,
                            which are cores and memory,
                            for the driver to use.
                            Additionally, data nodes on each machine,
                            and a name node or two on the master,
                            support the Hadoop Distributed File System.
                        </aside>
                    </script>
                    <audio>
                        <source src="media/stack.mp3" type="audio/mp3">
                    </audio>
                </section>
                <section data-markdown>
                    <script type="text/template">
                        ## HDFS
                        <img src="img/hdfs.svg" style="border-width: 0px; background-color: transparent; box-shadow: initial; float: right; margin-right: 2em; height:auto; max-width:50%;">

                        * Name Node has directory
                        * Data Node manages blocks
                        * like RAID

                        <aside class="notes" data-markdown>
                            The Hadoop Distributed File System (HDFS)
                            uses part of each Datanode's disk
                            to create a federated virtual disk,
                            managed by the Name Node.
                            The disk is organized in blocks,
                            usually 64, 128 or 512 megabytes,
                            which are distributed as needed.
                            Executor operations on local blocks
                            incur no network overhead.
                        </aside>
                    </script>
                    <audio>
                        <source src="media/hdfs.mp3" type="audio/mp3">
                    </audio>
                </section>
                <section data-markdown>
                    <script type="text/template">
                        ## RDD
                        <img src="img/rdd.svg" style="border-width: 0px; background-color: transparent; box-shadow: initial; float: right; margin-right: 2em; height:auto; max-width:50%;">

                        * immutable
                        * higher-order function API
                        * recipe
                        * spill to disk

                        <aside class="notes" data-markdown>
                            The primary data structure in Spark is the
                            Resilient Distributed Dataset or RDD.
                            Conceptually, it is a array or list of objects,
                            usually Plain Old Java Objects, or PoJos,
                            that exists in sets called partitions.
                            Partitions are distributed within executors,
                            and over the executors of the cluster on different machines.
                            Each RDD is immutable, which facilitates
                            parallelizing operations.
                            The RDD API is primarily higher-order functions,
                            that is, functions that take functions as parameters,
                            such as map, filter and fold.
                            In the example, we have a class X with just two members,
                            an id and a distance.
                            We get an immutable RDD 'a' of type X from somewhere,
                            and from that we can create a second immutable RDD, 'b',
                            by filtering 'a' to select objects that have
                            an id starting with "B".
                            The function passed as an argument is the
                            "underscore dot id dot startsWith B",
                            which is idiomatic Scala syntax for an anonymous function.
                            You can visualize that function being
                            wrapped in a closure and sent to each
                            executor that has one or more partitions of the RDD 'a',
                            and each executor then applying that function
                            to every element of the RDD to generate 'b'.
                            Some functions provide "repartition" options, but not filter,
                            so the results have the same partioning.
                            A third RDD 'c' is generated, this time of type Double,
                            by extracting the distance element from each object,
                            with the anonymous function "underscore dot dist".
                            Lastly, the distances are aggregated by the fold operation,
                            operating first over each partition,
                            and then second over the results for all the partitions.
                            Fold takes an initial value, that's the 0.0 in
                            the curried first argument to fold, and another anonymous
                            function "underscore plus underscore", which does the sum.
                            RDD resiliency comes from remembering
                            the sequence of these operations as a recipe,
                            that can be replayed as needed to recreate
                            one immutable RDD from its immutable precursors -
                            this is why immutability is important,
                            if the data had changed this would not be possible.
                            If memory becomes exhausted,
                            RDDs are discarded, keeping only their recipe.
                            Caching, or persisting, an RDD locks that RDD in memory.
                            In that case, if memory becomes exhausted,
                            the cached RDDs can spill to disk as serialized objects,
                            and be re-hydrated when required.
                        </aside>
                    </script>
                    <audio>
                        <source src="media/rdd.mp3" type="audio/mp3">
                    </audio>
                </section>
                <section data-markdown>
                    <script type="text/template">
                        ## Pair RDD
                        <img src="img/pair_rdd.svg" style="border-width: 0px; background-color: transparent; box-shadow: initial; float: right; margin-right: 2em; height:auto; max-width:50%;">

                        * Tuple2
                        * usually primary key
                        * RDD combining API, e.g.
                          * join
                          * leftOuterJoin
                          * cogroup

                        <aside class="notes" data-markdown>
                            Some special operations are only available
                            to RDD of Tuple2.
                            A tuple is a wrapping class, that groups
                            other objects and is supported by a bracket syntax in Scala.
                            A Tuple2 is two tuple with a key and a value,
                            similar to the Map-Reduce Key-Value construct.
                            In the example, the RDD of type X is transformed by
                            the keyBy function into a Tuple2 of types String and X,
                            wth the anonymous function "underscore dot id".
                            Often, the first element of the Tuple is the
                            primary key for each object.
                            Pair RDD methods revolve around combining RDDs in some way,
                            such as join, leftOuterJoin and cogroup,
                            which have similar meaning to the SQL operations of the same name.
                        </aside>
                    </script>
                    <audio>
                        <source src="media/pair.mp3" type="audio/mp3">
                    </audio>
                </section>
                <section data-markdown>
                    <script type="text/template">
                        ## Transformations & Actions
                        <img src="img/transformations_and_actions.svg" style="border-width: 0px; background-color: transparent; box-shadow: initial; float: right; margin-right: 2em; height:auto; max-width:50%;">

                        * transformations
                          * lazy & fast, in memory ~100ns
                          * map, filter, union
                        * actions
                          * eager & slow, over network ~500µs
                          * count, collect, fold

                        <aside class="notes" data-markdown>
                            All these Spark methods fall into two categories,
                            transformations and actions.
                            Transformatons are local operations,
                            where executors can operate independently,
                            requiring no interaction with other executors.
                            Evaluation of these is lazy,
                            that is, it's done only when necessary.
                            Mostly, these are in-memory operations,
                            and hence quite fast - on the order of nano-seconds.
                            These include map, filter and union.
                            Actions are, conversely, non-local operations,
                            where results need a cluster-wide concensus.
                            Evaluation is eager -
                            that is, they're done immediately upon being
                            encountered in the recipe -
                            so that other executors are not held up
                            waiting for a lazy evaluation.
                            These usually involve network traffic,
                            called a shuffle,
                            and hence are quite slow - on the order of micro-seconds.
                            These include count, collect and fold.
                            A lot of the tuning and optimization for Spark,
                            involves eliminating these shuffle operations,
                            either through different algorithms, different data structures
                            or reordering operations for parallelization.
                        </aside>
                    </script>
                    <audio>
                        <source src="media/transformations_and_actions.mp3" type="audio/mp3">
                    </audio>
                </section>
                <section data-markdown>
                    <script type="text/template">
                        ## CIMReader
                        <img src="img/CIMReader.svg" style="border-width: 0px; background-color: transparent; box-shadow: initial; height:auto;">

                        <aside class="notes" data-markdown>
                            The CIMReader primarily allows a Spark program to
                            read in one or more CIM files.
                            It produces RDD of CIM classes.
                            It also provides some additonal related functionality
                            such as:
                            - topological processing
                            - de-duplication
                            - and edge creation
                        </aside>
                    </script>
                    <audio>
                        <source src="media/CIMReader.mp3" type="audio/mp3">
                    </audio>
                </section>
                <section data-markdown>
                    <script type="text/template">
                        ## Class Hierarchy
                        <img src="img/nested.svg" style="border-width: 0px; background-color: transparent; box-shadow: initial; float: right; margin-right: 2em; height:auto; max-width:50%;">

                        * RDD per CIM class
                        * SQL Dataset
                        * hierarchically nested
                        * cached, Kyro serialized

                        <aside class="notes" data-markdown>
                            The CIMReader generates a globally named RDD per CIM class.
                            There are over 1500 such classes.
                            It also exposes these RDD as SQL Datasets
                            for Spark SQL’s optimized execution engine called Catalyst.
                            Each CIM subclass has its parent class encapsulated in it,
                            somewhat like Russian dolls.
                            In the example shown here, the Conducting Equipment superclass
                            is embedded in class Switch, both of which are in a named RDD.
                            This works up and down the hierarchy, so, one can access all switches,
                            independant of whether they are
                            Breaker, Disconnector, or Fuse subclasses by accessing the Switch RDD.
                            These RDD are cached, so the CIM file reading only occurs once.
                        </aside>
                    </script>
                    <audio>
                        <source src="media/hierarchy.mp3" type="audio/mp3">
                    </audio>
                </section>
                <section data-markdown>
                    <script type="text/template">
                        ## Use-Case – Asset Reports

                        ```SQL
                            select
                                t.ConductingEquipment.Equipment.PowerSystemResource.IdentifiedObject.mRID,
                                l.installationDate,
                                p.xPosition,
                                p.yPosition
                            from
                                Asset a,
                                LifecycleDate l,
                                PositionPoint p,
                                PowerTransformer t
                            where
                                t.ConductingEquipment.Equipment.PowerSystemResource.Location = p.Location and
                                p.sequenceNumber = 0 and
                                a.PowerSystemResources[0] = t.ConductingEquipment.Equipment.PowerSystemResource.IdentifiedObject.mRID and
                                a.lifecycle = l.BasicElement.mRID
                        ```

                        |    mRID|installationDate|    xPosition|    yPosition|
                        |--------|----------------|-------------|-------------|
                        |TRA12717|            2008|7.46252758059|46.9964952624|
                        | TRA2739|            1983|7.38499507626|46.9869053333|
                        | TRA7419|            1973|7.48111297055|46.9604453884|
                        |TRA17413|            2015|7.34330544480|47.0007609949|
                        |TRA12906|            2010|7.47843983575|46.9774132191|

                        <aside class="notes" data-markdown="">
                            This use case is a simple report of transformers,
                            their age and their spatial location.
                            This would be suitable for a map display
                            with colored dots for example.
                            We use SQL, but the same effect could be achieved
                            with Resilient Distributed Datasets (RDDs).
                            The advantage of SQL is that much of the Spark
                            optimization has been built into the Spark SQL
                            optimizer.
                            There are multiple joins over the PowerTransformer,
                            Asset, LifecycleDate, and PositionPoint tables.
                        </aside>
                    </script>
                    <audio>
                        <source src="media/use-case_asset.mp3" type="audio/mp3">
                    </audio>
                </section>
                <section data-markdown>
                    <script type="text/template">
                        ## GraphX & Pregel
                        <img src="img/pregel.svg" style="border-width: 0px; background-color: transparent; box-shadow: initial; float: right; margin-right: 2em; height:auto; max-width:50%;">

                        1. send initial message to all nodes
                        2. for each edge of receiving nodes:
                          * generate any new messages
                        3. merge multiple messages
                        4. send messages
                        5. repeat 2-4 until no messages

                        <aside class="notes" data-markdown>
                            An extremely useful API is provided by GraphX.
                            As its name implies,
                            it can construct a graph of edges and vertices of a network,
                            which can then be operated on by the Pregel algorithm.
                            The distributed Pregel algorithm,
                            in higher-order functional form,
                            takes three functions:
                            - a vertex updating function - to handle incoming messages to a vertex,
                            - a message generating function - propagate messages across edges,
                            - and a message merging function - to combine messages to the same vertex
                            The algorithm basically consists of
                            sending an initial message to every vertex,
                            then iterating over these three functions
                            until there are no new messages generated.
                        </aside>
                    </script>
                    <audio>
                        <source src="media/graphx.mp3" type="audio/mp3">
                    </audio>
                </section>
                <section data-markdown>
                    <script type="text/template">
                        ## Topological Nodes
                        <img src="img/topology_processor.svg" style="border-width: 0px; background-color: transparent; box-shadow: initial; float: right; margin-right: 2em; height:auto; max-width:50%;">

                        * propagate minimum vertex id
                        * update:
                          * RDD[TopologicalNode]
                          * RDD[Terminal]
                          * & superclasses
                        * topological islands
                          * all cables ⇐ 0Ω

                        <aside class="notes" data-markdown>
                            An example of Pregel processing is
                            the network topology processor identifying topological nodes,
                            that is, nodes at the same electric potential.
                            In this case the vertex data and message are simply a node id,
                            which is the hashCode of the element mRID - the primary key in CIM.
                            The vertex program handles the incoming message by keeping the
                            minimum vertex id.
                            The send message program sends a message to
                            any vertex with a greater vertex id,
                            but only if they are topologically connected,
                            either with a zero impedance cable or a closed switch.
                            The merge message program,
                            like the vertex program,
                            keeps the minimum vertex id.
                            When the algorithm terminates,
                            each vertex is labeled with the minimum vertex id
                            of all topologically connected vertecies,
                            which is used as a unique identifier for a topological node
                            of all the connected nodes.
                            The program then updates the TopologicalNode RDD and
                            the Terminal RDD and all superclasses.
                            Topological island processing is the same,
                            except all cables are considered zero impedance,
                            and the TopologicalIsland RDD is the one that's updated.
                        </aside>
                    </script>
                    <audio>
                        <source src="media/topological_processing.mp3" type="audio/mp3">
                    </audio>
                </section>
                <section data-markdown>
                    <script type="text/template">
                        ## Use-Case – Short Circuit Current
                        <img src="img/use-case_Short_Circuit.svg" style="border-width: 0px; background-color: transparent; box-shadow: initial; float: right; margin-right: 2em; height:auto; max-width:50%;">

                        * use Pregel
                        * label each house with:
                          * transformer
                          * impedance to station
                          * list of fuses
                        * use MV model, calculate:
                          * short circuit current
                          * fuse specificity

                        <aside class="notes" data-markdown>
                            This use-case is to determine the maximum current flow
                            under short circuit conditions at the house service,
                            and which fuse, if any, trips first.
                            It requires a model of the medium voltage network
                            under short-circuit conditions at the station.
                            The Pregel algorithm can be used to label each house service
                            with the supplying transformer, the total impedance to the station,
                            and the list of fuses encountered along the path
                            from the transformer to the house.
                            Then it apply some Ohms law and checks the fuse specificity
                            at every house.
                        </aside>
                    </script>
                    <audio>
                        <source src="media/use-case_short-circuit_current.mp3" type="audio/mp3">
                    </audio>
                </section>
                <section data-markdown>
                    <script type="text/template">
                        ## GridLAB-D
                        <img src="img/gridlabd.svg" style="border-width: 0px; background-color: transparent; box-shadow: initial; float: right; margin-right: 2em; height:auto; max-width:50%;">

                        * solve non-linear load flow
                          * glm: network model
                          * player: driving function
                          * recorder: observer
                        * open source
                        * http://www.gridlabd.org

                        <aside class="notes" data-markdown>
                            Where non-linear effects must be considered,
                            as is the case in most network analysis,
                            a load-flow or power-flow numerical analysis must be performed.
                            Programs that do this solve large sparse matrix equations
                            of the Thevenin or Norton equivalent circuit for a network,
                            and provide values or time-series for the variables in these equations,
                            such as cable current and node voltage.

                            Of the many programs considered, GridLAB-D was chosen because
                            - it is platform neutral, running on linux as well as Windows and OSX
                            - it needs no licence, allowing clusters to be expanded as needed
                            - it is open source, so there is the possibility of 
                              fixing show-stopper bugs, although one must know C/C++ to do that

                            The nomenclature used by GridLAB-D includes:

                            - glm - a file containing the network nodes, edges and configurations
                            - configuration - a model of a transformer or cable
                            - player - a Comma Separated Value (CSV) file of a boundary condition,
                              essentially a time-series driving function for a variable
                            - recorder - an output CSV file of the observed values of a
                              variable such as: power, voltage, current, phase angle etc.
                            - swing bus - infinite source or sink, a hard and fast
                              pinning of a supply variable, a built in boundary condition

                            When the program is invoked it reads in the glm model and the player files,
                            and produces recorder files for requested variables.
                        </aside>
                    </script>
                    <audio>
                        <source src="media/gridlabd.mp3" type="audio/mp3">
                    </audio>
                </section>
                <section data-markdown>
                    <script type="text/template">
                        ## Distributed GridLAB-D
                        <img src="img/solve.svg" style="border-width: 0px; background-color: transparent; box-shadow: initial; float: right; margin-right: 2em; height:auto; max-width:50%;">

                        * export topological islands
                        * RDD.pipe() ⟹ call native program (script) for each

                        <aside class="notes" data-markdown>
                            Since the entire network is too large to solve all at once,
                            the problem is partitioned into transformer service areas
                            (Trafokreise in German)
                            and these are solved individually.
                            Essentially, complete GridLAB-D packages for each topological
                            island are exported to the Hadoop Distributed File System,
                            and then these are solved in parallel.
                            The RDD pipe method is used, which basically bales out
                            to the operating system to execute a script for each element of an RDD,
                            in ths case the names of the Trafokreise.
                            This is where the distributed parallel multi-threaded world of Spark
                            boils down to the single-core, single-threaded
                            isolated world of GridLAB-D.
                            Since GridLAB-D knows nothing about Spark and HDFS,
                            the script first copies the .glm and player files
                            from HDFS to local disk,
                            executes the gridlabd program and then copies
                            the recorder files back to HDFS.
                            The recorder files are then slurped into Spark,
                            with the out-of-the-box CSV reader,
                            producing RDD of the recorder time series
                            double or complex values.
                        </aside>
                    </script>
                    <audio>
                        <source src="media/distributed_gridlabd.mp3" type="audio/mp3">
                    </audio>
                </section>
                <section data-markdown>
                    <script type="text/template">
                        ## Use-Case – Maximum Feed-In Power Precalculation
                        <img src="img/use-case_MaxFeedIn_Precalculation.svg" style="border-width: 0px; background-color: transparent; box-shadow: initial; float: right; margin-right: 2em; height:auto; max-width:50%;">

                        * use Pregel
                        * label each node with
                          * owning transformer
                          * smallest cable I<sub>rated</sub> on path
                          * path resistance
                        * for each house, find P<sub>max</sub> where:
                          * voltage exceeds +3% threshold
                          * current exceeds cable rating
                          * power exceeds transformer rating

                        <aside class="notes" data-markdown>
                            This use-case is: to determine the maximum power
                            for a new solar panel installation at each house. 
                            If there are no other power sources,
                            that is, no other photo-voltaic (PV) installations,
                            the problem is a linear problem that can be solved with GraphX.
                            So, ignoring any installed generators,
                            basically label each customer location with the
                            total line impedance from the supplying transformer,
                            and the minimum cable ampacity encountered along the path.
                            Then, knowing the transformer characteristic, it becomes an
                            Ohm's law calculation to determine what power level input would
                            lead to:
                            - excessive voltage
                            - excessive current
                            - or excessive transformer power
                            If there really are no existing generators, we're done.
                            Otherwise, this computed value is used as a best-case
                            input to the next step, which is simulation.
                        </aside>
                    </script>
                    <audio>
                        <source src="media/use-case_maximum_feed-in_power-precalculation.mp3" type="audio/mp3">
                    </audio>
                </section>
                <section data-markdown>
                    <script type="text/template">
                        ## Use-Case – Maximum Feed-In Power Simulation
                        <img src="img/use-case_MaxFeedIn_Simulation.svg" style="border-width: 0px; background-color: transparent; box-shadow: initial; float: right; margin-right: 2em; height:auto; max-width:50%;">

                        * export islands with existing PV
                        * execute gridlabd for each
                        * stepped negative load, time-multiplexed
                        * find P<sub>max</sub> where:
                          * voltage exceeds +3% threshold
                          * current exceeds cable rating
                          * power exceeds transformer rating
                        * two step sizes, 10kW and 1kW

                        <aside class="notes" data-markdown>
                            Where there are existing photo-voltaic installations
                            in a transformer service area
                            it is necessary to perform a load-flow analysis.
                            Each transformer service area that has existing PV
                            is exported and simulated using GridLAB-D.
                            It is assumed that existing PV are generating at maximum capacity.
                            To save time, the experiments for each house in a trafokreise are
                            time multilexed into one gridlabd execution -
                            the first house is analyzed in the first five minutes of simulation,
                            the second house in the next five, and so on.
                            At each house, during its experiment, a stepped
                            (negative) load is applied in the player file,
                            and otherwise the player is zero as shown in the upper graph.
                            Steps occur at regular intervals,
                            with the number of steps determined by the best-case value
                            from the precalculation done earlier with GraphX.
                            The recorder files from gridlabd which are:
                            - the time series of node voltages for each customer location
                            - the time series of the current in each cable
                            - the time series of power through the transformer
                            are time-demultiplexed out of the five minute windows and then
                            analyzed for the same excessive limits as in the precalculation.
                            By knowing the time at which an excess occurs,
                            the step - and hence the power - that caused it can be computed.
                            To save time and increase the resolution,
                            the simulation is actually done in two phases,
                            one of ten thousand Watts per step
                            and then a finer granularity phase of one thousand Watts per step.
                        </aside>
                    </script>
                    <audio>
                        <source src="media/use-case_maximum_feed-in_power-simulation.mp3" type="audio/mp3">
                    </audio>
                </section>
                <section data-markdown>
                    <script type="text/template">
                        ## CIM & Spark Summary

                        * high volume – compute large distribution networks
                        * cluster computing – AWS and Azure
                        * CIM deeply embedded – CIM data model used throughout
                        * linear and nonlinear use-cases – Pregel and GridLAB-D
                        * variety of languages and APIs

                        <aside class="notes" data-markdown>
                            In summary, Apache Spark is being used in
                            large distribution network computations
                            based on the CIM model and data file format.
                            It is being used on both AWS and Azure to solve
                            linear and non-linear use-cases, in Scala and R.
                        </aside>
                    </script>
                    <audio>
                        <source src="media/summary.mp3" type="audio/mp3">
                    </audio>
                </section>
                <section data-markdown>
                    <script type="text/template">
                      ## Questions

                      derrick.oswald@9code.ch
                    </script>
                </section>
			</div>
		</div>

		<script src="lib/js/head.min.js"></script>
		<script src="js/reveal.js"></script>

		<script>
			// More info about config & dependencies:
			// - https://github.com/hakimel/reveal.js#configuration
			// - https://github.com/hakimel/reveal.js#dependencies
			Reveal.initialize({
				autoPlayMedia: window.location.search == "?audio",
				autoSlide: window.location.search == "?audio" ? 7500 : 0,
				autoSlideMethod: Reveal.navigateRight,
				markdown: { gfm: true },
				width: "100%",
				height: "100%",
				margin: 0,
				minScale: 1,
				maxScale: 1,
				dependencies: [
					{ src: 'plugin/markdown/marked.js' },
					{ src: 'plugin/markdown/markdown.js' },
					{ src: 'plugin/notes/notes.js', async: true },
					{ src: 'plugin/highlight/highlight.js', async: true, callback: function() { hljs.initHighlightingOnLoad(); } }
				]
			});
		</script>
	</body>
</html>
